{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b09e3082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    "    NotebookLoader,\n",
    "    PyPDFLoader\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1498c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "436b8aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDfOnww_Cb2nzsccLYYRRBxy3SSWG-Cwd8\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7ecaea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_CODE_DIRECTORY = \"all_my_code\"\n",
    "# Path to the local vector database\n",
    "PERSIST_DIRECTORY = \"chroma_db\"\n",
    "# Chunking parameters\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80845328",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_documents(directory):\n",
    "    \"\"\"Loads all .py and .ipynb files from the specified directory.\"\"\"\n",
    "    documents = []\n",
    "    for item_path in Path(directory).rglob('*'):\n",
    "        if item_path.is_file():\n",
    "            \n",
    "            if item_path.suffix == '.pdf':\n",
    "                print(f\"Loading PDF: {item_path.name}\")\n",
    "                loader = PyPDFLoader(str(item_path))\n",
    "                documents.extend(loader.load())\n",
    "            \n",
    "            if item_path.suffix == '.py':\n",
    "                loader = TextLoader(str(item_path), encoding=\"utf-8\")\n",
    "                documents.extend(loader.load())\n",
    "            elif item_path.suffix == '.ipynb':\n",
    "                # NotebookLoader handles the JSON structure of .ipynb files\n",
    "                loader = NotebookLoader(\n",
    "                    str(item_path),\n",
    "                    include_outputs=False, # Don't include cell outputs\n",
    "                    max_output_length=20,\n",
    "                    remove_newline=True,\n",
    "                )\n",
    "                documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "\n",
    "def chunk_documents(documents):\n",
    "    \"\"\"\n",
    "    Chunks documents using a splitter that is aware of code syntax.\n",
    "    This is better than a simple character splitter.\n",
    "    \"\"\"\n",
    "    python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=\"python\", chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "    chunks = python_splitter.split_documents(documents)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50bf56f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents(SOURCE_CODE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa9c4694",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef310c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53951e46",
   "metadata": {},
   "source": [
    "gemini key = \"AIzaSyDfOnww_Cb2nzsccLYYRRBxy3SSWG-Cwd8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddb6abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=PERSIST_DIRECTORY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4efcb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## CHANGED ##: Import Google's chat and embedding classes\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ab585f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_15476\\1898709396.py:11: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. DEFINE CONSTANTS & INITIALIZE ---\n",
    "# ## CHANGED ##: Point to the new Gemini-powered database\n",
    "PERSIST_DIRECTORY = \"chroma_db\"\n",
    "# ## CHANGED ##: Use a Gemini Pro model for generation\n",
    "LLM_MODEL = \"gemini-1.5-flash-latest\"\n",
    "\n",
    "# --- 2. SETUP THE RAG CHAIN ---\n",
    "\n",
    "# ## CHANGED ##: Load the vector store using Google's embedding function\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "vectorstore = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings)\n",
    "\n",
    "# ## CHANGED ##: Initialize the Gemini LLM\n",
    "llm = ChatGoogleGenerativeAI(model=LLM_MODEL, temperature=0.1, convert_system_message_to_human=True)\n",
    "# Note: `convert_system_message_to_human=True` is sometimes needed for Gemini models\n",
    "# when using prompt templates that have a system message.\n",
    "\n",
    "# Create a retriever to fetch relevant documents\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# This prompt template is generic and works well with Gemini too.\n",
    "prompt_template = \"\"\"\n",
    "You are an expert researcg assistant and lab  mentor.\n",
    "Your task is to answer questions about the lab's database which is either code or other paper related information.\n",
    "Use the following retrieved context to answer the question.\n",
    "If you don't know the answer from the context, just say that you don't know.\n",
    "Be concise and provide code snippets from the context if they are relevant.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{input}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# This chain takes a question and the retrieved documents and generates an answer.\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# This is the final chain that combines the retriever and the question-answer chain.\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd6241a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\owner\\.conda\\envs\\lab_mentor\\lib\\site-packages\\langchain_google_genai\\chat_models.py:483: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"i want to make an optimizer, please send me the code\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa874bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'i want to make an optimizer, please send me the code',\n",
       " 'context': [Document(metadata={'source': 'all_my_code\\\\main_integrated.py'}, page_content='from sklearn.metrics import r2_score\\nfrom skopt.learning import GaussianProcessRegressor\\nfrom skopt.learning.gaussian_process.kernels import ConstantKernel, Matern\\nfrom sklearn.neighbors import KNeighborsRegressor\\nimport random\\nimport math\\nimport numpy as np\\nnp.random.seed(237)\\nimport matplotlib.pyplot as plt\\nfrom skopt.plots import plot_gaussian_process\\nfrom skopt.optimizer import Optimizer \\nimport pandas as pd'),\n",
       "  Document(metadata={'source': 'all_my_code\\\\main_integrated.py'}, page_content='from sklearn.metrics import r2_score\\nfrom skopt.learning import GaussianProcessRegressor\\nfrom skopt.learning.gaussian_process.kernels import ConstantKernel, Matern\\nfrom sklearn.neighbors import KNeighborsRegressor\\nimport random\\nimport math\\nimport numpy as np\\nnp.random.seed(237)\\nimport matplotlib.pyplot as plt\\nfrom skopt.plots import plot_gaussian_process\\nfrom skopt.optimizer import Optimizer \\nimport pandas as pd'),\n",
       "  Document(metadata={'source': 'all_my_code\\\\main_integrated.py'}, page_content='matern_fixed = ConstantKernel(1.0, constant_value_bounds=\\'fixed\\') * Matern(\\n    length_scale=np.ones(1), length_scale_bounds=\\'fixed\\', nu=2.5)\\nmatern_tunable = ConstantKernel(1.0, (1e-5, 1e5)) * Matern(length_scale=1.0, length_scale_bounds=(1e-5, 1e5), nu=2.5)\\n\\nbounds = [(1,20),(1,15),(2.5,2.6)]    \\n#defining regressors \\ngpr = GaussianProcessRegressor(kernel=matern_fixed)   \\n\\ntfrs= []\\nrs = []\\ncs = []\\ndifs = []\\nopter_min =  Optimizer(bounds,base_estimator=gpr,n_initial_points=3,acq_func=\"EI\",random_state=np.random.randint(1200))\\ngap = 10\\ni=0\\ndif = 1000\\nwhile dif>gap:\\n    print(i)\\n    asked0 = opter_min.ask()\\n    dif = flow(asked0,run=0)\\n    opter_min.tell(asked0,dif)\\n    print(asked0)\\n    print(dif)\\n    tfrs.append(asked0[0])\\n    rs.append(asked0[1])\\n    cs.append(asked0[2])\\n    difs.append(dif)\\n    dc = {\"tfr\":tfrs,\"r\":rs,\"c\":cs,\"dif\":difs}\\n    df = pd.DataFrame(dc)\\n    df.to_csv(\"round\"+str(i)+\".csv\")\\n    i=i+1\\n    print(\"round finished\")')],\n",
       " 'answer': '```python\\nfrom skopt.optimizer import Optimizer\\nfrom skopt.learning import GaussianProcessRegressor\\nfrom skopt.learning.gaussian_process.kernels import ConstantKernel, Matern\\n\\nmatern_fixed = ConstantKernel(1.0, constant_value_bounds=\\'fixed\\') * Matern(\\n    length_scale=np.ones(1), length_scale_bounds=\\'fixed\\', nu=2.5)\\n\\nbounds = [(1,20),(1,15),(2.5,2.6)]    \\ngpr = GaussianProcessRegressor(kernel=matern_fixed)   \\n\\nopter_min =  Optimizer(bounds,base_estimator=gpr,n_initial_points=3,acq_func=\"EI\",random_state=np.random.randint(1200))\\n```\\n\\nThis code snippet shows the creation of an `Optimizer` object.  Remember that you need to define the `flow` function (used in the `while` loop in the provided context) for this to work completely.  The `flow` function takes a parameter set as input and returns a value that the optimizer uses to minimize.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4169762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_mentor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
